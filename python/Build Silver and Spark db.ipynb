{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Build Spark Database and Tables\r\n",
        "\r\n",
        "## This code will read the CSV files in bronze, create the parquet files in Silver and then build the serverless spark db and tables based from the silver layer.\r\n",
        "\r\n",
        "\r\n",
        "##### Nick Tinsley\r\n",
        "##### Sr. Cloud Solution Architect\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Set Global Variables\r\n",
        "### dbname is passed in by pipeline parameter, if running manually hard code the db name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "dbname = \"ccc_poc\"\r\n",
        "storage_account = \"dltinsynapse\" \r\n",
        "container_name = \"datalake\"\r\n",
        "dl_path_bronze =\"datalake/elt/bronze\"\r\n",
        "dl_path_silver = \"datalake/elt/silver\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run VariableNotebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "varA + varB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      },
      "source": [
        "\r\n",
        "spark.conf.set(\"myapp.dbname\",dbname)\r\n",
        "bronze_file_path = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/{dl_path_bronze}/{dbname}\"\r\n",
        "silver_file_path = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/{dl_path_silver}/{dbname}\"\r\n",
        "\r\n",
        "print(bronze_file_path)\r\n",
        "print(silver_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from notebookutils import mssparkutils\r\n",
        "# import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Read CSV files from pipeline and build Parquet files for Lake House"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "#get list of file names\r\n",
        "#https://.blob.core.windows.net/datalake\r\n",
        "file_list = mssparkutils.fs.ls(bronze_file_path)\r\n",
        "\r\n",
        "\r\n",
        "#loop through each csv file\r\n",
        "for items in file_list:\r\n",
        "    print(items.name)\r\n",
        "    print(items.path)\r\n",
        "    \r\n",
        "    #load into temporary data frame\r\n",
        "    tempdf = spark.read.load(items.path, format='csv', header=True)\r\n",
        "\r\n",
        "    # pandatestpath = silver_file_path+f\"/padatest/{items.name}\"\r\n",
        "    # print(f\"creating {pandatestpath}\")\r\n",
        "\r\n",
        "    # pandaTest = tempdf.toPandas()\r\n",
        "    # pandaTest.to_csv(pandatestpath, mode='w', index=False, header=True)\r\n",
        "    # print(f\"finished writing {pandatestpath}\")\r\n",
        "\r\n",
        "    ##testing out pandas stuff\r\n",
        "    # #build output path string\r\n",
        "    filename = items.name.strip(\".csv\")\r\n",
        "    parquet_path = f\"{silver_file_path}/{filename}\"\r\n",
        "    \r\n",
        "    # #write to parquet files in output string\r\n",
        "    print(f\"Writing parquet file to: {parquet_path}\")\r\n",
        "    tempdf.write.mode(\"overwrite\").parquet(parquet_path)\r\n",
        "    print(f\"Finished writing {parquet_path}\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create Spark DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "DROP DATABASE IF EXISTS ${myapp.dbname} CASCADE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "-- create serverless database${myapp.dbname}\r\n",
        "CREATE DATABASE ${myapp.dbname}\r\n",
        "-- drop DATABASE mncrash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Save Parquet Files to Spark Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "silver_files = mssparkutils.fs.ls(silver_file_path)\r\n",
        "\r\n",
        "for items in silver_files:\r\n",
        "    print(items.name)\r\n",
        "    # print(items.path + \"//*.snappy.parquet\")\r\n",
        "\r\n",
        "    path = items.path + \"/*.snappy.parquet\"\r\n",
        "\r\n",
        "    silverdf = spark.read.load(path, format=\"parquet\")\r\n",
        "    # silverdf.show(1)\r\n",
        "    silverdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f'{dbname}.{items.name}')\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}